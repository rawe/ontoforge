services:
  neo4j:
    image: neo4j:5
    container_name: ontoforge-neo4j
    ports:
      # External ports differ from dev (7474/7687) to avoid conflicts
      - "17474:7474"
      - "17687:7687"
    environment:
      NEO4J_AUTH: neo4j/ontoforge_dev
    volumes:
      - neo4j-data:/data
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "ontoforge_dev", "RETURN 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  backend:
    build: ../backend
    container_name: ontoforge-backend
    ports:
      - "8000:8000"
    environment:
      # DB_URI uses Docker-internal networking — not affected by external port mapping
      DB_URI: bolt://neo4j:7687
      DB_USER: neo4j
      DB_PASSWORD: ontoforge_dev
      # Semantic search (optional — remove comments to enable)
      # EMBEDDING_PROVIDER: ollama
      # EMBEDDING_MODEL: nomic-embed-text
      # If Ollama runs on the host machine:
      # EMBEDDING_BASE_URL: http://host.docker.internal:11434
      # If Ollama runs as a container in this compose (see ollama service below):
      # EMBEDDING_BASE_URL: http://ollama:11434
    depends_on:
      neo4j:
        condition: service_healthy

  frontend:
    build: ../frontend
    container_name: ontoforge-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend

  # Ollama embedding provider (optional — uncomment to run Ollama in Docker)
  # After first start, pull the model: docker exec ollama ollama pull nomic-embed-text
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ontoforge-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama

volumes:
  neo4j-data:
  # ollama-data:
